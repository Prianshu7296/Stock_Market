{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949f815a",
   "metadata": {},
   "source": [
    "This notebook demonstrates exploratory data analysis, feature engineering, and comparison of multiple classification models on historical stock data. It displays the main pipeline and various plots to clarify results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e71f6-93bb-45e6-812f-c489ce0571c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, roc_curve, auc,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "# Load config\n",
    "with open(r\"D:/Stock_Market/configs/config.yaml\", \"r\") as f:  # Use full path\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "data_path = config[\"data_path\"]\n",
    "target = config[\"target_column\"]  # For binary/classification\n",
    "test_size = config[\"test_size\"]\n",
    "random_seed = config[\"random_seed\"]\n",
    "model_dir = config[\"model_dir\"]\n",
    "results_dir = config[\"results_dir\"]\n",
    "model_cfg = config[\"models\"]\n",
    "\n",
    "df = pd.read_excel(data_path)\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fadd78e-70c9-4a3f-921a-ca73ee94a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix - Raw\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc9725-760c-4d54-a7b6-35f696c1c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [col for col in df.columns if col != target]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(df[feat_cols]), columns=feat_cols)\n",
    "df_scaled = X_scaled.copy()\n",
    "df_scaled[target] = df[target]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cca262-69df-4304-99f8-bb62b348fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [col for col in df.columns if col != target]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(df[feat_cols]), columns=feat_cols)\n",
    "df_scaled = X_scaled.copy()\n",
    "df_scaled[target] = df[target]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66b964-05fd-4c38-baca-39d77f0de92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df_scaled.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix - Standardized\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250c878-1967-4a29-80e1-fc9b741942c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def vif_sklearn(df):\n",
    "    vif_data = []\n",
    "    X = df.values\n",
    "    feature_names = df.columns\n",
    "    for i in range(X.shape[1]):\n",
    "        y = X[:, i]\n",
    "        X_not_i = X[:, np.arange(X.shape[1]) != i]\n",
    "        r_sq = LinearRegression().fit(X_not_i, y).score(X_not_i, y)\n",
    "        vif = 1.0 / (1.0 - r_sq) if r_sq < 1.0 else np.inf\n",
    "        vif_data.append(vif)\n",
    "    return pd.DataFrame({'feature': feature_names, 'VIF': vif_data})\n",
    "\n",
    "def drop_high_vif(df, threshold=10.0):\n",
    "    features = df.columns.tolist()\n",
    "    while True:\n",
    "        vif_df = vif_sklearn(df[features])\n",
    "        max_vif = vif_df[\"VIF\"].max()\n",
    "        if max_vif > threshold:\n",
    "            drop_feat = vif_df.loc[vif_df[\"VIF\"] == max_vif, \"feature\"].values[0]\n",
    "            features.remove(drop_feat)\n",
    "            print(f\"Dropped {drop_feat} (VIF={max_vif:.2f})\")\n",
    "        else:\n",
    "            break\n",
    "    return df[features]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75dc870-b0a8-4f0f-ae81-c8e51155aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VIF Feature Selection and Outlier Removal Block ---\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def vif_sklearn(df):\n",
    "    vif_data = []\n",
    "    X = df.values\n",
    "    feature_names = df.columns\n",
    "    for i in range(X.shape[1]):\n",
    "        y = X[:, i]\n",
    "        X_not_i = X[:, np.arange(X.shape[1]) != i]\n",
    "        r_sq = LinearRegression().fit(X_not_i, y).score(X_not_i, y)\n",
    "        vif = 1.0 / (1.0 - r_sq) if r_sq < 1.0 else np.inf\n",
    "        vif_data.append(vif)\n",
    "    return pd.DataFrame({'feature': feature_names, 'VIF': vif_data})\n",
    "\n",
    "def drop_high_vif(df, threshold=10.0):\n",
    "    features = df.columns.tolist()\n",
    "    while True:\n",
    "        vif_df = vif_sklearn(df[features])\n",
    "        max_vif = vif_df[\"VIF\"].max()\n",
    "        if max_vif > threshold:\n",
    "            drop_feat = vif_df.loc[vif_df[\"VIF\"] == max_vif, \"feature\"].values[0]\n",
    "            features.remove(drop_feat)\n",
    "            print(f\"Dropped {drop_feat} (VIF={max_vif:.2f})\")\n",
    "        else:\n",
    "            break\n",
    "    return df[features]\n",
    "\n",
    "def remove_outliers(df, cols, t=5.0):\n",
    "    Q1 = df[cols].quantile(0.25)\n",
    "    Q3 = df[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = ~((df[cols] < (Q1 - t*IQR)) | (df[cols] > (Q3 + t*IQR))).any(axis=1)\n",
    "    return df[mask]\n",
    "\n",
    "# Instantiate working features dataframe WITHOUT missing (NaN) values\n",
    "features = [col for col in df_scaled.columns if col != target]\n",
    "df_features_clean = df_scaled[features].dropna()  # drop rows with NaN only for VIF\n",
    "\n",
    "# VIF Feature Selection\n",
    "X_vif = drop_high_vif(df_features_clean, threshold=10.0)\n",
    "print(\"Columns after VIF:\", list(X_vif.columns))\n",
    "\n",
    "# Merge target back (only for rows kept by VIF process)\n",
    "X_all = X_vif.copy()\n",
    "X_all[target] = df_scaled.loc[X_vif.index, target]\n",
    "\n",
    "# Outlier removal (IQR filter)\n",
    "X_clean = remove_outliers(X_all, X_all.columns, t=5.0).dropna()\n",
    "print(\"Shape after outlier removal:\", X_clean.shape)\n",
    "\n",
    "# Now continue with train/test split, modeling, etc., using X_clean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87766795-eb66-4b0c-bd97-55296a71f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, cols, t=5.0):\n",
    "    Q1 = df[cols].quantile(0.25)\n",
    "    Q3 = df[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return df[~((df[cols] < Q1 - t*IQR) | (df[cols] > Q3 + t*IQR)).any(axis=1)]\n",
    "\n",
    "X = X_vif.copy()\n",
    "X[target] = df_scaled[target]\n",
    "X = remove_outliers(X, X.columns, t=5.0).dropna()\n",
    "print(\"Shape after outlier removal:\", X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94063f-9e25-4eb1-ae23-55f31034f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X[target]\n",
    "X = X.drop(columns=[target])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_seed, stratify=y if y.nunique() <= 10 else None\n",
    ")\n",
    "print(\"Train shape:\", X_train.shape, \"| Test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(**model_cfg[\"logistic_regression\"]),\n",
    "    \"Random Forest\": RandomForestClassifier(**model_cfg[\"random_forest\"]),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(**model_cfg[\"gradient_boosting\"]),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(**model_cfg[\"decision_tree\"]),\n",
    "    \"SVM\": SVC(\n",
    "        probability=True,\n",
    "        kernel=model_cfg[\"svm\"][\"binary\"][\"kernel\"][0],\n",
    "        C=model_cfg[\"svm\"][\"binary\"][\"C\"][0],\n",
    "        gamma=model_cfg[\"svm\"][\"binary\"][\"gamma\"][0])\n",
    "}\n",
    "\n",
    "if y.nunique() <= 10:\n",
    "    plt.figure(figsize=(8,7))\n",
    "    auc_dict = {}\n",
    "    for name, model in clf_models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:,1] if hasattr(model, \"predict_proba\") else None\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        if y_proba is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            auc_val = auc(fpr, tpr)\n",
    "            auc_dict[name] = auc_val\n",
    "            plt.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.2f})\")\n",
    "\n",
    "    plt.plot([0,1],[0,1], 'k--', label=\"Random (AUC=0.5)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"ROC Curve Comparison\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb70556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "regression_target = \"EPS\"   # Change as needed, or from YAML!\n",
    "if regression_target in df_scaled.columns:\n",
    "    # Apply VIF feature selection for regression (same logic as classification)\n",
    "    features_reg = [col for col in df_scaled.columns if col != regression_target]\n",
    "    df_features_reg_clean = df_scaled[features_reg].dropna()  # drop rows with NaN for VIF\n",
    "    \n",
    "    # VIF Feature Selection for regression\n",
    "    X_reg_vif = drop_high_vif(df_features_reg_clean, threshold=10.0)\n",
    "    print(\"Columns after VIF filtering for regression:\", list(X_reg_vif.columns))\n",
    "    \n",
    "    # Merge target back (only for rows kept by VIF process)\n",
    "    X_reg_all = X_reg_vif.copy()\n",
    "    X_reg_all[regression_target] = df_scaled.loc[X_reg_vif.index, regression_target]\n",
    "    \n",
    "    # Outlier removal (IQR filter) for regression\n",
    "    X_reg_clean = remove_outliers(X_reg_all, X_reg_all.columns, t=5.0).dropna()\n",
    "    print(\"Shape after outlier removal for regression:\", X_reg_clean.shape)\n",
    "    \n",
    "    # Separate features and target after all cleaning\n",
    "    y_reg_clean = X_reg_clean[regression_target]\n",
    "    X_reg_final = X_reg_clean.drop(columns=[regression_target])\n",
    "    \n",
    "    # Scale features for SVR (critical for good performance)\n",
    "    scaler = StandardScaler()\n",
    "    X_reg_scaled = pd.DataFrame(scaler.fit_transform(X_reg_final), \n",
    "                               columns=X_reg_final.columns, \n",
    "                               index=X_reg_final.index)\n",
    "\n",
    "    # Train/test split on final cleaned data\n",
    "    Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "        X_reg_scaled, y_reg_clean, test_size=test_size, random_state=random_seed\n",
    "    )\n",
    "\n",
    "    # Define regression models with safe parameter access\n",
    "    reg_models = {\n",
    "        \"Random Forest\": RandomForestRegressor(**model_cfg[\"random_forest\"]),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(**model_cfg[\"gradient_boosting\"]),\n",
    "        \"Decision Tree\": DecisionTreeRegressor(**model_cfg[\"decision_tree\"]),\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"SVR\": SVR(\n",
    "            kernel=model_cfg[\"svm\"][\"regression\"].get(\"kernel\", [\"rbf\"])[0],\n",
    "            C=model_cfg[\"svm\"][\"regression\"].get(\"C\", [1.0])[0],\n",
    "            gamma=model_cfg[\"svm\"][\"regression\"].get(\"gamma\", [\"scale\"])[0],\n",
    "            epsilon=model_cfg[\"svm\"][\"regression\"].get(\"epsilon\", [0.1])[0]\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Train and evaluate all regressors\n",
    "    for name, model in reg_models.items():\n",
    "        model.fit(Xr_train, yr_train)\n",
    "        y_pred_r = model.predict(Xr_test)\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(\"R2 score:\", r2_score(yr_test, y_pred_r))\n",
    "        print(\"MAE:\", mean_absolute_error(yr_test, y_pred_r))\n",
    "        print(\"MSE:\", mean_squared_error(yr_test, y_pred_r))\n",
    "\n",
    "        # Actual vs Predicted\n",
    "        plt.scatter(yr_test, y_pred_r, alpha=0.7)\n",
    "        plt.title(f\"{name} - Actual vs Predicted\")\n",
    "        plt.xlabel(\"Actual\")\n",
    "        plt.ylabel(\"Predicted\")\n",
    "        plt.show()\n",
    "\n",
    "        # Residuals\n",
    "        residuals = yr_test - y_pred_r\n",
    "        plt.scatter(y_pred_r, residuals, alpha=0.7)\n",
    "        plt.title(f\"{name} - Residuals Plot\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.show()\n",
    "        plt.hist(residuals, bins=30, color='orange')\n",
    "        plt.xlabel(\"Residuals\")\n",
    "        plt.title(f\"{name} - Residuals Distribution\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486798a6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
